{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Author:-aam35\n",
    "Analyzing Forgetting in neural networks\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()\n",
    "\n",
    "# random seed to get the consistent result\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "## Permuted MNIST\n",
    "## a training set of 55,000 examples, validation is 5000, and a test set of 10,000 examples\n",
    "data = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "## parameters\n",
    "num_tasks_to_run = 10\n",
    "num_epochs_per_task = 20\n",
    "minibatch_size = 64\n",
    "learning_rate = 0.0001\n",
    "num_train = len(data.train.labels)\n",
    "num_test = len(data.test.labels)\n",
    "\n",
    "# Generate the tasks specifications as a list of random permutations of the input pixels.\n",
    "#  permuting the pixels in all images with the same permutation\n",
    "# for training\n",
    "train_permutation = []\n",
    "# for validation\n",
    "validation_permutation = []\n",
    "# for test\n",
    "test_permutation = []\n",
    "# permutation\n",
    "permutation = []\n",
    "for task in range(num_tasks_to_run):\n",
    "    ## 28*28 pixels\n",
    "    permutation.append(np.random.RandomState(seed=task*(42)).permutation(784))\n",
    "    #train_permutation.append(np.random.RandomState(seed=task*(42)).permutation(data.train.images))\n",
    "    #validation_permutation.append(np.random.RandomState(seed=task*(42)).permutation(data.validation.images))\n",
    "    #test_permutation.append(np.random.RandomState(seed=task*(42)).permutation(data.test.images))\n",
    "\n",
    "train_permutation.append(data.train.images[:,permutation[task]])\n",
    "test_permutation.append(data.test.images[:,permutation[task]])\n",
    "\n",
    "#Based on tutorial provided create your MLP model for above problem\n",
    "#For TF2.0 users Keras can be used for loading trainable variables and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_permutation[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 1\n",
    "size_input = 784 # MNIST data input (img shape: 28*28)\n",
    "size_hidden = 256\n",
    "size_output = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Define class to build mlp model\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
    "        size_input, size_hidden, size_output, device\n",
    "    \n",
    "        # Initialize weights between input layer and hidden layer\n",
    "        self.W1 = tfe.Variable(tf.random_normal([self.size_input, self.size_hidden]))\n",
    "        #self.W1 = tf.get_variable(name=\"W1\", shape=(784,128),dtype=tf.float32)\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b1 = tfe.Variable(tf.random_normal([1, self.size_hidden]))\n",
    "        #self.b1 = tf.get_variable(name=\"b1\", shape=(128, ),dtype=tf.float32)\n",
    "        # Initialize weights between hidden layer and output layer\n",
    "        self.W2 = tfe.Variable(tf.random_normal([self.size_hidden, self.size_output]))\n",
    "        #self.W2 = tf.get_variable(name=\"W2\", shape=(128,10),dtype=tf.float32)\n",
    "        # Initialize biases for output layer\n",
    "        self.b2 = tfe.Variable(tf.random_normal([1, self.size_output]))\n",
    "        #self.b2 = tf.get_variable(name=\"b2\", shape=(10, ),dtype=tf.float32)\n",
    "    \n",
    "\n",
    "        \n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.b1, self.b2]\n",
    "        # optimizer\n",
    "        # Test with SGD,Adam, RMSProp\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # prediction\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "      \n",
    "        return self.y\n",
    "    \n",
    "    ## loss function\n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
    "        #return tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(y_pred_tf), reduction_indices=[1]))\n",
    "        \n",
    "  \n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        # optimizer\n",
    "        # Test with SGD,Adam, RMSProp\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        \n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        #Remember to normalize your dataset before moving forward\n",
    "        # Compute values in hidden layer\n",
    "        what = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        hhat = tf.nn.relu(what)\n",
    "        # dropout\n",
    "        d_hhat = tf.nn.dropout(hhat, 0.5)\n",
    "        # Compute output\n",
    "        output = tf.matmul(hhat, self.W2) + self.b2\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "        #Second add tf.Softmax(output) and then return this variable\n",
    "        #print(output)\n",
    "        #return tf.nn.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Number of Epoch = 1 - loss:= 0.0372\n",
      "Training Accuracy = 0.10645454376935959\n",
      "Number of Epoch = 2 - loss:= 0.0357\n",
      "Training Accuracy = 0.10541818290948868\n",
      "Number of Epoch = 3 - loss:= 0.0343\n",
      "Training Accuracy = 0.10529091209173203\n",
      "Number of Epoch = 4 - loss:= 0.0332\n",
      "Training Accuracy = 0.10407272726297379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5f5dfba65f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m            \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m            \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m            \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Epoch = {} - loss:= {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m        \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f3873fb94363>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         self.optimizer.apply_gradients(zip(grads, self.variables),\n\u001b[0;32m---> 81\u001b[0;31m                               global_step=tf.train.get_or_create_global_step())\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m           \u001b[0mupdate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, default_name, values)\u001b[0m\n\u001b[1;32m   6030\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_eager_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6031\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_symbolic_input_in_eager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6032\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_eager_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6033\u001b[0m       \u001b[0;31m# The presence of a graph tensor in `self._values` overrides the context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6034\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Initialize model using CPU\n",
    "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
    "\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "Ptest_dataset_images = []\n",
    "Ptest_dataset_labels = []\n",
    "\n",
    "# training process\n",
    "for run in range(num_tasks_to_run):\n",
    "    # train for 50 epochs for task1\n",
    "    if run == 0:\n",
    "        num_epochs_per_task = 30\n",
    "        #num_epochs_per_task = 10\n",
    "    else :\n",
    "        #num_epochs_per_task = 20\n",
    "        num_epochs_per_task = 10\n",
    "    for epoch in range(num_epochs_per_task):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_permutation[run], data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "           .shuffle(buffer_size=1000)\\\n",
    "           .batch(batch_size=minibatch_size)\n",
    "        loss_total = tfe.Variable(0, dtype=tf.float32)\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_on_cpu.forward(inputs)\n",
    "            loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "            mlp_on_cpu.backward(inputs, outputs)\n",
    "        print('Number of Epoch = {} - loss:= {:.4f}'.format(epoch + 1, loss_total.numpy() / num_train))\n",
    "        preds = mlp_on_cpu.compute_output(data.train.images)\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(data.train.labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print (\"Training Accuracy = {}\".format(accuracy.numpy()))\n",
    " \n",
    "    # accumulate test dataset\n",
    "    Ptest_dataset_images.extend(test_permutation[run])\n",
    "    Ptest_dataset_labels.extend(data.test.labels)\n",
    "    \n",
    "    # test accuracy\n",
    "    preds = mlp_on_cpu.compute_output(Ptest_dataset_images)\n",
    "    #preds = mlp_on_cpu.compute_output(data.test.images)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(data.test.labels, 1))\n",
    "    #Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # To keep sizes compatible with model\n",
    "    print (\"Accuracy = {}\".format(accuracy.numpy()))\n",
    "\n",
    "        \n",
    "time_taken = time.time() - time_start\n",
    "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#For per epoch_time = Total_Time / Number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 2\n",
    "size_input = 784 # MNIST data input (img shape: 28*28)\n",
    "size_hidden_1 = 256\n",
    "size_hidden_2 = 256\n",
    "size_output = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Define class to build mlp model\n",
    "class MLP1(object):\n",
    "    def __init__(self, size_input, size_hidden_1, size_hidden_2, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden_1, self.size_hidden_2, self.size_output, self.device =\\\n",
    "        size_input, size_hidden_1, size_hidden_2, size_output, device\n",
    "    \n",
    "        # Initialize weights between input layer and hidden layer1\n",
    "        self.W1 = tfe.Variable(tf.random_normal([self.size_input, self.size_hidden_1]))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b1 = tfe.Variable(tf.random_normal([1, self.size_hidden_1]))\n",
    "        # Initialize weights between input layer and hidden layer\n",
    "        self.W2 = tfe.Variable(tf.random_normal([self.size_hidden_1, self.size_hidden_2]))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b2 = tfe.Variable(tf.random_normal([1, self.size_hidden_2]))\n",
    "        # Initialize weights between hidden layer and output layer\n",
    "        self.W3 = tfe.Variable(tf.random_normal([self.size_hidden_2, self.size_output]))\n",
    "        # Initialize biases for output layer\n",
    "        self.b3 = tfe.Variable(tf.random_normal([1, self.size_output]))\n",
    "        \n",
    "\n",
    "    \n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        \n",
    "    \n",
    "    # prediction\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "      \n",
    "        return self.y\n",
    "    \n",
    "    \n",
    "    ## loss function\n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
    "        #return tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(y_pred_tf), reduction_indices=[1]))\n",
    "        \n",
    "  \n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        # optimizer\n",
    "        # Test with SGD,Adam, RMSProp\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        \n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # Remember to normalize your dataset before moving forward\n",
    "        # Compute values in hidden layer\n",
    "        what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        hhat1 = tf.nn.relu(what1)\n",
    "        # dropout\n",
    "        d_hhat = tf.nn.dropout(hhat1, 0.3)\n",
    "        # Compute output\n",
    "        what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "        hhat2 = tf.nn.relu(what2)\n",
    "        #d_hhat = tf.nn.dropout(hhat1, 0.3)\n",
    "        output = tf.matmul(hhat2, self.W3) + self.b3\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "        #Second add tf.Softmax(output) and then return this variable\n",
    "        #print(output)\n",
    "        return tf.nn.softmax(output)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch = 1 - loss:= 0.0369\n",
      "Training Accuracy = 0.10154545307159424\n",
      "Number of Epoch = 2 - loss:= 0.0369\n",
      "Training Accuracy = 0.10143636167049408\n",
      "Number of Epoch = 3 - loss:= 0.0369\n",
      "Training Accuracy = 0.10152727365493774\n",
      "Number of Epoch = 4 - loss:= 0.0369\n",
      "Training Accuracy = 0.1035090908408165\n",
      "Number of Epoch = 5 - loss:= 0.0369\n",
      "Training Accuracy = 0.10170909017324448\n",
      "Number of Epoch = 6 - loss:= 0.0369\n",
      "Training Accuracy = 0.10221818089485168\n",
      "Number of Epoch = 7 - loss:= 0.0369\n",
      "Training Accuracy = 0.10167272388935089\n",
      "Number of Epoch = 8 - loss:= 0.0369\n",
      "Training Accuracy = 0.1019272729754448\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb203f9dd653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m            \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m            \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m            \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Epoch = {} - loss:= {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-582b1f731578>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0my_pred_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;31m#return tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(y_pred_tf), reduction_indices=[1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits_v2_helper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2469\u001b[0m     \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m     cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 2471\u001b[0;31m         precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[0;31m# The output cost shape should be the input minus axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   7842\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7843\u001b[0m         \u001b[0;34m\"SoftmaxCrossEntropyWithLogits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7844\u001b[0;31m         features, labels)\n\u001b[0m\u001b[1;32m   7845\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Initialize model using CPU\n",
    "mlp_on_cpu = MLP1(size_input, size_hidden_1, size_hidden_2, size_output, device='cpu')\n",
    "\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "Ptest_dataset_images = []\n",
    "Ptest_dataset_labels = []\n",
    "\n",
    "# training process\n",
    "for run in range(num_tasks_to_run):\n",
    "    # train for 50 epochs for task1\n",
    "    if run == 0:\n",
    "        num_epochs_per_task = 30\n",
    "        #num_epochs_per_task = 10\n",
    "    else :\n",
    "        #num_epochs_per_task = 20\n",
    "        num_epochs_per_task = 10\n",
    "    for epoch in range(num_epochs_per_task):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_permutation[run], data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "           .shuffle(buffer_size=1000)\\\n",
    "           .batch(batch_size=minibatch_size)\n",
    "        loss_total = tfe.Variable(0, dtype=tf.float32)\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_on_cpu.forward(inputs)\n",
    "            loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "            mlp_on_cpu.backward(inputs, outputs)\n",
    "        print('Number of Epoch = {} - loss:= {:.4f}'.format(epoch + 1, loss_total.numpy() / num_train))\n",
    "        preds = mlp_on_cpu.compute_output(train_permutation[run])\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(data.train.labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print (\"Training Accuracy = {}\".format(accuracy.numpy()))\n",
    " \n",
    "    # accumulate test dataset\n",
    "    Ptest_dataset_images.extend(test_permutation[run])\n",
    "    Ptest_dataset_labels.extend(data.test.labels)\n",
    "    \n",
    "    # test accuracy\n",
    "    preds = mlp_on_cpu.compute_output(Ptest_dataset_images)\n",
    "    #preds = mlp_on_cpu.compute_output(data.test.images)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Ptest_dataset_labels, 1))\n",
    "    #Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # To keep sizes compatible with model\n",
    "    print (\"Accuracy = {}\".format(accuracy.numpy()))\n",
    "\n",
    "        \n",
    "time_taken = time.time() - time_start\n",
    "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#For per epoch_time = Total_Time / Number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
