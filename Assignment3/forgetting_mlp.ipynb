{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-8caafb9f8551>:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Author:-aam35\n",
    "Analyzing Forgetting in neural networks\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()\n",
    "\n",
    "# random seed to get the consistent result\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "## Permuted MNIST\n",
    "## a training set of 55,000 examples, validation is 5000, and a test set of 10,000 examples\n",
    "data = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "## parameters\n",
    "num_tasks_to_run = 10\n",
    "num_epochs_per_task = 20\n",
    "minibatch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_train = len(data.train.labels)\n",
    "num_test = len(data.test.labels)\n",
    "\n",
    "# Generate the tasks specifications as a list of random permutations of the input pixels.\n",
    "#  permuting the pixels in all images with the same permutation\n",
    "# for training\n",
    "train_permutation = []\n",
    "# for validation\n",
    "validation_permutation = []\n",
    "# for test\n",
    "test_permutation = []\n",
    "for task in range(num_tasks_to_run):\n",
    "    ## 28*28 pixels\n",
    "    train_permutation.append(np.random.RandomState(seed=task*(42)).permutation(data.train.images))\n",
    "    validation_permutation.append(np.random.RandomState(seed=task*(42)).permutation(data.validation.images))\n",
    "    test_permutation.append(np.random.RandomState(seed=task*(42)).permutation(data.test.images))\n",
    "\n",
    "    \n",
    "\n",
    "#Based on tutorial provided create your MLP model for above problem\n",
    "#For TF2.0 users Keras can be used for loading trainable variables and dataset.\n",
    "#You might need google collab to run large scale experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 1\n",
    "size_input = 784 # MNIST data input (img shape: 28*28)\n",
    "size_hidden = 256\n",
    "size_output = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Define class to build mlp model\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
    "        size_input, size_hidden, size_output, device\n",
    "    \n",
    "        # Initialize weights between input layer and hidden layer\n",
    "        self.W1 = tfe.Variable(tf.random_normal([self.size_input, self.size_hidden]))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b1 = tfe.Variable(tf.random_normal([1, self.size_hidden]))\n",
    "        # Initialize weights between hidden layer and output layer\n",
    "        self.W2 = tfe.Variable(tf.random_normal([self.size_hidden, self.size_output]))\n",
    "        # Initialize biases for output layer\n",
    "        self.b2 = tfe.Variable(tf.random_normal([1, self.size_output]))\n",
    "    \n",
    "\n",
    "        \n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.b1, self.b2]\n",
    "        \n",
    "    \n",
    "    # prediction\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "      \n",
    "        return self.y\n",
    "    \n",
    "    ## loss function\n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
    "        #return tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(y_pred_tf), reduction_indices=[1]))\n",
    "        \n",
    "  \n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        # optimizer\n",
    "        # Test with SGD,Adam, RMSProp\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        \n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        #Remember to normalize your dataset before moving forward\n",
    "        # Compute values in hidden layer\n",
    "        what = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        hhat = tf.nn.relu(what)\n",
    "        # dropout\n",
    "        d_hhat = tf.nn.dropout(hhat, 0.3)\n",
    "        # Compute output\n",
    "        output = tf.matmul(hhat, self.W2) + self.b2\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "        #Second add tf.Softmax(output) and then return this variable\n",
    "        #print(output)\n",
    "        return tf.nn.softmax(output)\n",
    "        #return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Number of Epoch = 1 - loss:= 0.0363\n",
      "Training Accuracy = 0.17096363008022308\n",
      "Number of Epoch = 2 - loss:= 0.0355\n",
      "Training Accuracy = 0.19650909304618835\n",
      "Number of Epoch = 3 - loss:= 0.0353\n",
      "Training Accuracy = 0.20694544911384583\n",
      "Number of Epoch = 4 - loss:= 0.0352\n",
      "Training Accuracy = 0.2173818200826645\n",
      "Number of Epoch = 5 - loss:= 0.0350\n",
      "Training Accuracy = 0.22594545781612396\n",
      "Number of Epoch = 6 - loss:= 0.0349\n",
      "Training Accuracy = 0.23739999532699585\n",
      "Number of Epoch = 7 - loss:= 0.0347\n",
      "Training Accuracy = 0.24912726879119873\n",
      "Number of Epoch = 8 - loss:= 0.0345\n",
      "Training Accuracy = 0.2635818123817444\n",
      "Number of Epoch = 9 - loss:= 0.0342\n",
      "Training Accuracy = 0.28141817450523376\n",
      "Number of Epoch = 10 - loss:= 0.0339\n",
      "Training Accuracy = 0.30212727189064026\n",
      "Number of Epoch = 11 - loss:= 0.0336\n",
      "Training Accuracy = 0.31778180599212646\n",
      "Number of Epoch = 12 - loss:= 0.0334\n",
      "Training Accuracy = 0.33347272872924805\n",
      "Number of Epoch = 13 - loss:= 0.0332\n",
      "Training Accuracy = 0.3461090922355652\n",
      "Number of Epoch = 14 - loss:= 0.0330\n",
      "Training Accuracy = 0.3585818111896515\n",
      "Number of Epoch = 15 - loss:= 0.0328\n",
      "Training Accuracy = 0.36903637647628784\n",
      "Number of Epoch = 16 - loss:= 0.0326\n",
      "Training Accuracy = 0.37909090518951416\n",
      "Number of Epoch = 17 - loss:= 0.0325\n",
      "Training Accuracy = 0.38790908455848694\n",
      "Number of Epoch = 18 - loss:= 0.0323\n",
      "Training Accuracy = 0.39559999108314514\n",
      "Number of Epoch = 19 - loss:= 0.0322\n",
      "Training Accuracy = 0.4046909213066101\n",
      "Number of Epoch = 20 - loss:= 0.0321\n",
      "Training Accuracy = 0.4111636281013489\n",
      "Number of Epoch = 21 - loss:= 0.0320\n",
      "Training Accuracy = 0.4166363775730133\n",
      "Number of Epoch = 22 - loss:= 0.0319\n",
      "Training Accuracy = 0.4222727417945862\n",
      "Number of Epoch = 23 - loss:= 0.0318\n",
      "Training Accuracy = 0.4278181791305542\n",
      "Number of Epoch = 24 - loss:= 0.0317\n",
      "Training Accuracy = 0.4344363510608673\n",
      "Number of Epoch = 25 - loss:= 0.0316\n",
      "Training Accuracy = 0.440890908241272\n",
      "Number of Epoch = 26 - loss:= 0.0316\n",
      "Training Accuracy = 0.4455636441707611\n",
      "Number of Epoch = 27 - loss:= 0.0315\n",
      "Training Accuracy = 0.4513818323612213\n",
      "Number of Epoch = 28 - loss:= 0.0314\n",
      "Training Accuracy = 0.455454558134079\n",
      "Number of Epoch = 29 - loss:= 0.0313\n",
      "Training Accuracy = 0.4588181674480438\n",
      "Number of Epoch = 30 - loss:= 0.0313\n",
      "Training Accuracy = 0.4620545506477356\n",
      "Accuracy = 0.46480000019073486\n",
      "Number of Epoch = 1 - loss:= 0.0312\n",
      "Training Accuracy = 0.46619999408721924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-60f610167efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m        \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m            \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m            \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \"\"\"\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1832\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1834\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1835\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Initialize model using CPU\n",
    "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
    "\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "Ptest_dataset_images = []\n",
    "Ptest_dataset_labels = []\n",
    "\n",
    "# training process\n",
    "for run in range(num_tasks_to_run):\n",
    "    # train for 50 epochs for task1\n",
    "    if run == 0:\n",
    "        num_epochs_per_task = 30\n",
    "        #num_epochs_per_task = 10\n",
    "    else :\n",
    "        #num_epochs_per_task = 20\n",
    "        num_epochs_per_task = 10\n",
    "    for epoch in range(num_epochs_per_task):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "           .shuffle(buffer_size=1000)\\\n",
    "           .batch(batch_size=minibatch_size)\n",
    "        loss_total = tfe.Variable(0, dtype=tf.float32)\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_on_cpu.forward(inputs)\n",
    "            loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "            mlp_on_cpu.backward(inputs, outputs)\n",
    "        print('Number of Epoch = {} - loss:= {:.4f}'.format(epoch + 1, loss_total.numpy() / num_train))\n",
    "        preds = mlp_on_cpu.compute_output(data.train.images)\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(data.train.labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print (\"Training Accuracy = {}\".format(accuracy.numpy()))\n",
    " \n",
    "    # accumulate test dataset\n",
    "    #Ptest_dataset_images.extend(test_permutation[run])\n",
    "    #Ptest_dataset_labels.extend(data.test.labels)\n",
    "    \n",
    "    # test accuracy\n",
    "    #preds = mlp_on_cpu.compute_output(Ptest_dataset_images)\n",
    "    preds = mlp_on_cpu.compute_output(data.test.images)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(data.test.labels, 1))\n",
    "    #Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # To keep sizes compatible with model\n",
    "    print (\"Accuracy = {}\".format(accuracy.numpy()))\n",
    "\n",
    "        \n",
    "time_taken = time.time() - time_start\n",
    "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#For per epoch_time = Total_Time / Number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 2\n",
    "size_input = 784 # MNIST data input (img shape: 28*28)\n",
    "size_hidden_1 = 256\n",
    "size_hidden_2 = 256\n",
    "size_output = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Define class to build mlp model\n",
    "class MLP1(object):\n",
    "    def __init__(self, size_input, size_hidden_1, size_hidden_2, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden_1, self.size_hidden_2, self.size_output, self.device =\\\n",
    "        size_input, size_hidden_1, size_hidden_2, size_output, device\n",
    "    \n",
    "        # Initialize weights between input layer and hidden layer1\n",
    "        self.W1 = tfe.Variable(tf.random_normal([self.size_input, self.size_hidden_1]))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b1 = tfe.Variable(tf.random_normal([1, self.size_hidden_1]))\n",
    "        # Initialize weights between input layer and hidden layer\n",
    "        self.W2 = tfe.Variable(tf.random_normal([self.size_hidden_1, self.size_hidden_2]))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b2 = tfe.Variable(tf.random_normal([1, self.size_hidden_2]))\n",
    "        # Initialize weights between hidden layer and output layer\n",
    "        self.W3 = tfe.Variable(tf.random_normal([self.size_hidden_2, self.size_output]))\n",
    "        # Initialize biases for output layer\n",
    "        self.b3 = tfe.Variable(tf.random_normal([1, self.size_output]))\n",
    "        \n",
    "\n",
    "    \n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        \n",
    "    \n",
    "    # prediction\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "      \n",
    "        return self.y\n",
    "    \n",
    "    \n",
    "    ## loss function\n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
    "        #return tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(y_pred_tf), reduction_indices=[1]))\n",
    "        \n",
    "  \n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        # optimizer\n",
    "        # Test with SGD,Adam, RMSProp\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        \n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # Remember to normalize your dataset before moving forward\n",
    "        # Compute values in hidden layer\n",
    "        what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        hhat1 = tf.nn.relu(what1)\n",
    "        # dropout\n",
    "        d_hhat = tf.nn.dropout(hhat1, 0.3)\n",
    "        # Compute output\n",
    "        what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "        hhat2 = tf.nn.relu(what2)\n",
    "        #d_hhat = tf.nn.dropout(hhat1, 0.3)\n",
    "        output = tf.matmul(hhat2, self.W3) + self.b3\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "        #Second add tf.Softmax(output) and then return this variable\n",
    "        #print(output)\n",
    "        return tf.nn.softmax(output)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch = 1 - loss:= 0.0369\n",
      "Training Accuracy = 0.10154545307159424\n",
      "Number of Epoch = 2 - loss:= 0.0369\n",
      "Training Accuracy = 0.10143636167049408\n",
      "Number of Epoch = 3 - loss:= 0.0369\n",
      "Training Accuracy = 0.10152727365493774\n",
      "Number of Epoch = 4 - loss:= 0.0369\n",
      "Training Accuracy = 0.1035090908408165\n",
      "Number of Epoch = 5 - loss:= 0.0369\n",
      "Training Accuracy = 0.10170909017324448\n",
      "Number of Epoch = 6 - loss:= 0.0369\n",
      "Training Accuracy = 0.10221818089485168\n",
      "Number of Epoch = 7 - loss:= 0.0369\n",
      "Training Accuracy = 0.10167272388935089\n",
      "Number of Epoch = 8 - loss:= 0.0369\n",
      "Training Accuracy = 0.1019272729754448\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb203f9dd653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m            \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m            \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m            \u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Epoch = {} - loss:= {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-582b1f731578>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0my_pred_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;31m#return tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(y_pred_tf), reduction_indices=[1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits_v2_helper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2469\u001b[0m     \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m     cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 2471\u001b[0;31m         precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[0;31m# The output cost shape should be the input minus axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   7842\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7843\u001b[0m         \u001b[0;34m\"SoftmaxCrossEntropyWithLogits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7844\u001b[0;31m         features, labels)\n\u001b[0m\u001b[1;32m   7845\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Initialize model using CPU\n",
    "mlp_on_cpu = MLP1(size_input, size_hidden_1, size_hidden_2, size_output, device='cpu')\n",
    "\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "Ptest_dataset_images = []\n",
    "Ptest_dataset_labels = []\n",
    "\n",
    "# training process\n",
    "for run in range(num_tasks_to_run):\n",
    "    # train for 50 epochs for task1\n",
    "    if run == 0:\n",
    "        num_epochs_per_task = 30\n",
    "        #num_epochs_per_task = 10\n",
    "    else :\n",
    "        #num_epochs_per_task = 20\n",
    "        num_epochs_per_task = 10\n",
    "    for epoch in range(num_epochs_per_task):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_permutation[run], data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "           .shuffle(buffer_size=1000)\\\n",
    "           .batch(batch_size=minibatch_size)\n",
    "        loss_total = tfe.Variable(0, dtype=tf.float32)\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_on_cpu.forward(inputs)\n",
    "            loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "            mlp_on_cpu.backward(inputs, outputs)\n",
    "        print('Number of Epoch = {} - loss:= {:.4f}'.format(epoch + 1, loss_total.numpy() / num_train))\n",
    "        preds = mlp_on_cpu.compute_output(train_permutation[run])\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(data.train.labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print (\"Training Accuracy = {}\".format(accuracy.numpy()))\n",
    " \n",
    "    # accumulate test dataset\n",
    "    Ptest_dataset_images.extend(test_permutation[run])\n",
    "    Ptest_dataset_labels.extend(data.test.labels)\n",
    "    \n",
    "    # test accuracy\n",
    "    preds = mlp_on_cpu.compute_output(Ptest_dataset_images)\n",
    "    #preds = mlp_on_cpu.compute_output(data.test.images)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Ptest_dataset_labels, 1))\n",
    "    #Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # To keep sizes compatible with model\n",
    "    print (\"Accuracy = {}\".format(accuracy.numpy()))\n",
    "\n",
    "        \n",
    "time_taken = time.time() - time_start\n",
    "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#For per epoch_time = Total_Time / Number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
