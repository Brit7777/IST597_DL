{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-7699d7170da3>:21: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"CNN_week9.ipynb\n",
    "\n",
    "IST597 :- Implementing CNN from scratch\n",
    "Week 9 Tutorial\n",
    "\n",
    "Author:- aam35\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()\n",
    "seed = 1234\n",
    "tf.random.set_random_seed(seed=seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"data/fashion/\", one_hot=True, reshape=False)\n",
    "\n",
    "batch_size = 64\n",
    "hidden_size = 100\n",
    "learning_rate = 0.01\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self,hidden_size,output_size,device=None):\n",
    "        filter_h, filter_w, filter_c , filter_n = 5 ,5 ,1 ,30\n",
    "        self.W1 = tf.Variable(tf.random_normal([filter_h, filter_w, filter_c, filter_n], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([filter_n]),dtype=tf.float32)\n",
    "        self.W2 = tf.Variable(tf.random_normal([14*14*filter_n, hidden_size], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([hidden_size]),dtype=tf.float32)\n",
    "        self.W3 = tf.Variable(tf.random_normal([hidden_size, output_size], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.zeros([output_size]),dtype=tf.float32)\n",
    "        self.variables = [self.W1,self.b1, self.W2, self.b2, self.W3, self.b3]\n",
    "        self.device = device\n",
    "        self.size_output = output_size\n",
    "  \n",
    "    def flatten(self,X, window_h, window_w, window_c, out_h, out_w, stride=1, padding=0):\n",
    "    \n",
    "        X_padded = tf.pad(X, [[0,0], [padding, padding], [padding, padding], [0,0]])\n",
    "\n",
    "        windows = []\n",
    "        for y in range(out_h):\n",
    "            for x in range(out_w):\n",
    "                window = tf.slice(X_padded, [0, y*stride, x*stride, 0], [-1, window_h, window_w, -1])\n",
    "                windows.append(window)\n",
    "            stacked = tf.stack(windows) # shape : [out_h, out_w, n, filter_h, filter_w, c]\n",
    "\n",
    "        return tf.reshape(stacked, [-1, window_c*window_w*window_h])\n",
    "  \n",
    "    def convolution(self,X, W, b, padding, stride):\n",
    "        n, h, w, c = map(lambda d: d.value, X.get_shape())\n",
    "        #print(X.get_shape())\n",
    "        #print(data.train.images.get_shape())\n",
    "        filter_h, filter_w, filter_c, filter_n = [d.value for d in W.get_shape()]\n",
    "    \n",
    "        out_h = (h + 2*padding - filter_h)//stride + 1\n",
    "        out_w = (w + 2*padding - filter_w)//stride + 1\n",
    "\n",
    "        X_flat = self.flatten(X, filter_h, filter_w, filter_c, out_h, out_w, stride, padding)\n",
    "        W_flat = tf.reshape(W, [filter_h*filter_w*filter_c, filter_n])\n",
    "    \n",
    "        z = tf.matmul(X_flat, W_flat) + b     # b: 1 X filter_n\n",
    "    \n",
    "        return tf.transpose(tf.reshape(z, [out_h, out_w, n, filter_n]), [2, 0, 1, 3])\n",
    "    \n",
    " \n",
    "    \n",
    "    def relu(self,X):\n",
    "        return tf.maximum(X, tf.zeros_like(X))\n",
    "    \n",
    "    def max_pool(self,X, pool_h, pool_w, padding, stride):\n",
    "        n, h, w, c = [d.value for d in X.get_shape()]\n",
    "    \n",
    "        out_h = (h + 2*padding - pool_h)//stride + 1\n",
    "        out_w = (w + 2*padding - pool_w)//stride + 1\n",
    "\n",
    "        X_flat = self.flatten(X, pool_h, pool_w, c, out_h, out_w, stride, padding)\n",
    "\n",
    "        pool = tf.reduce_max(tf.reshape(X_flat, [out_h, out_w, n, pool_h*pool_w, c]), axis=3)\n",
    "        return tf.transpose(pool, [2, 0, 1, 3])\n",
    "\n",
    "    \n",
    "    def affine(self,X, W, b):\n",
    "        n = X.get_shape()[0].value # number of samples\n",
    "        X_flat = tf.reshape(X, [n, -1])\n",
    "        return tf.matmul(X_flat, W) + b \n",
    "    \n",
    "    def softmax(self,X):\n",
    "        X_centered = X - tf.reduce_max(X) # to avoid overflow\n",
    "        X_exp = tf.exp(X_centered)\n",
    "        exp_sum = tf.reduce_sum(X_exp, axis=1)\n",
    "        return tf.transpose(tf.transpose(X_exp) / exp_sum) \n",
    "    \n",
    "  \n",
    "    def cross_entropy_error(self,yhat, y):\n",
    "        return -tf.reduce_mean(tf.log(tf.reduce_sum(yhat * y, axis=1)))\n",
    "    \n",
    "    \n",
    "    def batch_norm(self, X):\n",
    "       # mini_batch_mean = X.sum(axis=0) / len(X)\n",
    "        print(X.shape)\n",
    "        mini_batch_mean = tf.reduce_mean(X)\n",
    "        #print(mini_batch_mean)\n",
    "        #mini_batch_var = ((X-mini_batch_mean) ** 2).sum(axis=0) / len(X)\n",
    "        mini_batch_var = tf.reduce_mean(tf.square(mini_batch_mean))\n",
    "        #print(mini_batch_var)\n",
    "        result = (X - mini_batch_mean) / ((mini_batch_var + 1e-8) ** 0.5)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "  \n",
    "    def batch_norm_tensor(self, X):\n",
    "        result = tf.nn.batch_normalization(x, mean = x.mean(axis=0),\n",
    "                                 variance = x.var(axis=0),\n",
    "                                 offset = 0.0, scale = 1.0,\n",
    "                                 variance_epsilon = le-8)\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def weight_normal(self, X):\n",
    "        return 1\n",
    "    \n",
    "    def layer_normal(self, X):\n",
    "        return 1\n",
    "    \n",
    "    # apply batch normalization here\n",
    "    def forward(self,X):\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "      \n",
    "        return self.y\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
    "    \n",
    "    \n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "          backward pass\n",
    "        \"\"\"\n",
    "      # optimizer\n",
    "      # Test with SGD,Adam, RMSProp\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "      #predicted = self.forward(X_train)\n",
    "      #current_loss = self.loss(predicted, y_train)\n",
    "      #optimizer.minimize(current_loss, self.variables)\n",
    "\n",
    "      #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "      #print(predicted)\n",
    "      #print(current_loss)\n",
    "      #current_loss_tf = tf.cast(current_loss, dtype=tf.float32)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "      \n",
    "      \n",
    "    def compute_output(self,X):\n",
    "        conv_layer1 = self.convolution(X, self.W1, self.b1, padding=2, stride=1)\n",
    "        conv_activation = self.relu(conv_layer1)\n",
    "        # conv = (64, 28, 28, 30)\n",
    "        #print(conv_activation)\n",
    "        conv_normal = self.batch_norm(conv_activation)\n",
    "        conv_pool = self.max_pool(conv_normal, pool_h=2, pool_w=2, padding=0, stride=2)\n",
    "        conv_affine =self.affine(conv_pool, self.W2,self.b2)\n",
    "        conv_affine_activation = self.relu(conv_affine)\n",
    "      \n",
    "        conv_affine_1 = self.affine(conv_affine_activation, self.W3, self.b3)\n",
    "        return conv_affine_1\n",
    "\n",
    "    def accuracy_function(yhat,true_y):\n",
    "        correct_prediction = tf.equal(tf.argmax(yhat, 1), tf.argmax(true_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n",
      "(64, 28, 28, 30)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model using GPU\n",
    "mlp_on_cpu = CNN(hidden_size,output_size, device='cpu')\n",
    "\n",
    "num_epochs = 4\n",
    "train_x =  tf.convert_to_tensor(data.train.images)\n",
    "train_y = tf.convert_to_tensor(data.train.labels)\n",
    "time_start = time.time()\n",
    "num_train = 55000\n",
    "z= 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "           .shuffle(buffer_size=1000)\\\n",
    "           .batch(batch_size=batch_size)\n",
    "        loss_total = tf.Variable(0, dtype=tf.float32)\n",
    "        accuracy_total = tf.Variable(0, dtype=tf.float32)\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_on_cpu.forward(inputs)\n",
    "            loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "#             accuracy_train = accuracy_function(preds,outputs)\n",
    "#             accuracy_total = accuracy_total + accuracy_train\n",
    "            mlp_on_cpu.backward(inputs, outputs)\n",
    "            #print(z)\n",
    "            #z = z+ 1\n",
    "        print('Number of Epoch = {} - loss:= {:.4f}'.format(epoch + 1, loss_total.numpy() / num_train))\n",
    "        preds = mlp_on_cpu.compute_output(train_x)\n",
    "        accuracy_train = accuracy_function(preds,train_y)\n",
    "        \n",
    "        accuracy_train = accuracy_train * 100\n",
    "        print (\"Training Accuracy = {}\".format(accuracy_train.numpy()))\n",
    "        \n",
    "        \n",
    "#         preds_val = mlp_on_cpu.compute_output(data.validation.images)\n",
    "#         accuracy_val = accuracy_function(preds_val,data.validation.labels)\n",
    "#         accuracy_val = accuracy_val * 100\n",
    "#         print (\"Validation Accuracy = {}\".format(accuracy_val.numpy()))\n",
    " \n",
    "#test accuracy\n",
    "test_x =  tf.convert_to_tensor(data.test.images)\n",
    "test_y = tf.convert_to_tensor(data.test.labels)\n",
    "preds_test = mlp_on_cpu.compute_output(test_x)\n",
    "accuracy_test = accuracy_function(preds_test,test_y)\n",
    "# To keep sizes compatible with model\n",
    "accuracy_test = accuracy_test * 100\n",
    "print (\"Test Accuracy = {}\".format(accuracy_test.numpy()))\n",
    "\n",
    "        \n",
    "# time_taken = time.time() - time_start\n",
    "# print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "# #For per epoch_time = Total_Time / Number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
